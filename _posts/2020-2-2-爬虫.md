---
layout: post
title: '第一个爬虫'
date: 2020-02-02
categories: 技术
cover: 'http://on2171g4d.bkt.clouddn.com/jekyll-theme-h2o-postcover.jpg'
tags: 爬虫 python
typora-root-url: ..
---

这几天女朋友拜托我爬取一些论文需要的数据。虽然我python和爬虫都不会，不过结合网上的教程和例子，还是用两天解决了。

## 前期准备

语言选择的是python 3.7，最简单的安装方法是使用[Anaconda](https://www.anaconda.com/)科学包。

使用的库分别有**selenium**、**openpyxl**和**pdfplumber**。

* **selenium**是一个web自动化测试工具。我们可以用它获取到动态网页中的elements

* **openpyxl**用于操作excel。

* 在测试多个pdf库后，**pdfplumber**能够较好的获取到pdf中的汉字。

## 网页分析

我要爬取的网页是所有关键字为*辞职*的公告原文，并判断辞职原因。

首先，我们要获取所有公告的url，然后将所有公告下载下来并获取其中数据。

这里我们通过该[网址](http://www.cninfo.com.cn/new/commonUrl/pageOfSearch?url=disclosure/list/search&keywords=辞职#szseMain)获得下面四列，其中第三列为超链接。


![](assets/img/image-20200202193208153.png)

接下来，获取公告pdf的url

![](assets\img\image-20200202194702339.png)

最后解析pdf，从中获得辞职原因。

### 具体代码

这部分主要是获取所有公告

``` python
# 浏览器驱动
from selenium import webdriver
# 用python操作excel
import openpyxl
# 操作时间
import time

# 创建一个workbook
wb = openpyxl.Workbook()
# 创建一个sheet
wb.create_sheet('data')
# 保存
wb.save('cases.xlsx')
wb = openpyxl.load_workbook('cases.xlsx')
activeSheet = wb['data']
driver_path = '/home/j/Downloads/chromedriver'
# 设置webdriver的路径为driver_path
driver = webdriver.Chrome(executable_path=driver_path)
driver.get(
    "http://www.cninfo.com.cn/new/commonUrl/pageOfSearch?url=disclosure/list/search&keywords=%E8%BE%9E%E8%81%8C&startDate=2013-10-19&endDate=2015-04-30#sseMain")
# print(driver.page_source)
num = 1  # excel行数
page = '1'  # 网页页数
while (page != '55'):
    #判断当前页数
    page = driver.find_element_by_class_name('number.active').text
    submitTag = driver.find_element_by_tag_name('tbody')
    submitTag1 = submitTag.find_elements_by_class_name('el-table_1_column_1')
    submitTag2 = submitTag.find_elements_by_class_name('el-table_1_column_2')
    submitTag3 = submitTag.find_elements_by_class_name('el-table_1_column_3')
    submitTag4 = submitTag.find_elements_by_class_name('el-table_1_column_4')
    # 将code,name,link,time录入到excel
    for code, name, link, timee in zip(submitTag1, submitTag2, submitTag3, submitTag4):
        num = num + 1
        cell_1 = activeSheet.cell(row=num, column=1)
        cell_2 = activeSheet.cell(row=num, column=2)
        cell_3 = activeSheet.cell(row=num, column=3)
        cell_4 = activeSheet.cell(row=num, column=4)
        cell_5 = activeSheet.cell(row=num, column=5)
        cell_1.value = code.text
        cell_2.value = name.text
        herf = link.find_element_by_link_text(link.text)
        cell_3.value = '=HYPERLINK("{}", "{}")'.format(herf.get_attribute("href"), herf.text)
        cell_4.value = timee.text
        cell_5.value = herf.get_attribute("href")
    driver.find_element_by_class_name('btn-next').click()
    time.sleep(2)
wb.save('cases.xlsx')
driver.close()
```

接下来从公告网页中获取pdf，获得其中的文字并查询辞职原因
``` python
# -*- coding: utf-8 -*-
from selenium import webdriver
import openpyxl
import requests
import re
import pdfplumber


# pdf链接转txt
def pdfurl2txt(requests_pdf_url):
    # 下载pdf
    r = requests.get(requests_pdf_url)
    filename = 'temp.pdf'
    with open(filename, 'wb+') as f:
        f.write(r.content)
    pdf_input = pdfplumber.open(filename)
    page = pdf_input.pages[0]
    text = page.extract_text()
    # print(text)
    return text


# set chromdriver path
driver_path = '/home/j/Downloads/chromedriver'
driver = webdriver.Chrome(executable_path=driver_path)
wb = openpyxl.load_workbook('cases.xlsx')
activeSheet = wb['data']
# 遍历所有行
num = 2
while True:
    # 获取pdfurl
    cell = activeSheet.cell(row=num, column=5)
    if cell.value == '':
        break
    url = cell.value
    pdfurl = ''
    driver.get(url)
    className = r'a.page-filedetail-fullscreen'
    tag_col = driver.find_element_by_class_name('cols.col-md-4')
    # 网页中直接显示公告
    if  tag_col.get_attribute('style'):
        className = r'page-detail-article'
        txt = driver.find_element_by_class_name(className).text
    # 网页中pdf显示公告
    else:
        submitTag = driver.find_element_by_tag_name(className)
        pdfurl = submitTag.get_attribute("href")
        if pdfurl == None:
            continue
        txt = pdfurl2txt(pdfurl)
    # 去除txt中的换行符号
    if isinstance(txt,str):
        txt = txt.replace('\n', '')
    else:
        txt=''
    #正则表达式
    s = r'\u56e0[\u4e00-\u9fa5]{1,10}\u539f\u56e0'
    #处理匹配失败的情况
    try:
        match = re.findall(s, txt)[0]
    except:
        match = ''
    print(match)
    cell_2 = activeSheet.cell(row=num, column=6)
    cell_3 = activeSheet.cell(row=num, column=7)
    cell_4 = activeSheet.cell(row=num, column=8)
    cell_2.value = match
    cell_3.value = txt
    cell_4.value = pdfurl
    print(str(num) + ' ' + pdfurl)
    num = num + 1
    wb.save('cases.xlsx')
```

至此，我们就可以爬取到所有数据了。